{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2acbeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator , img_to_array, load_img\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4eab905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this to install mediapipe for the first time\n",
    "#!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c3555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "\n",
    "\n",
    "class HandDetector:\n",
    "    \"\"\"\n",
    "    Finds Hands using the mediapipe library. Exports the landmarks\n",
    "    in pixel format. Adds extra functionalities like finding how\n",
    "    many fingers are up or the distance between two fingers. Also\n",
    "    provides bounding box info of the hand found.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode=False, maxHands=2, detectionCon=0.5, minTrackCon=0.5):\n",
    "        \"\"\"\n",
    "        :param mode: In static mode, detection is done on each image: slower\n",
    "        :param maxHands: Maximum number of hands to detect\n",
    "        :param detectionCon: Minimum Detection Confidence Threshold\n",
    "        :param minTrackCon: Minimum Tracking Confidence Threshold\n",
    "        \"\"\"\n",
    "        self.mode = mode\n",
    "        self.maxHands = maxHands\n",
    "        self.detectionCon = detectionCon\n",
    "        self.minTrackCon = minTrackCon\n",
    "\n",
    "        self.mpHands = mp.solutions.hands\n",
    "        self.hands = self.mpHands.Hands(static_image_mode=self.mode, max_num_hands=self.maxHands,\n",
    "                                        min_detection_confidence=self.detectionCon,\n",
    "                                        min_tracking_confidence=self.minTrackCon)\n",
    "        self.mpDraw = mp.solutions.drawing_utils\n",
    "        self.tipIds = [4, 8, 12, 16, 20]\n",
    "        self.fingers = []\n",
    "        self.lmList = []\n",
    "\n",
    "    def findHands(self, img, draw=True, flipType=True):\n",
    "        \"\"\"\n",
    "        Finds hands in a BGR image.\n",
    "        :param img: Image to find the hands in.\n",
    "        :param draw: Flag to draw the output on the image.\n",
    "        :return: Image with or without drawings\n",
    "        \"\"\"\n",
    "        #blank_image = np.zeros((1080,1080,3), np.uint8)\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        self.results = self.hands.process(imgRGB)\n",
    "        allHands = []\n",
    "        h, w, c = img.shape\n",
    "        if self.results.multi_hand_landmarks:\n",
    "            for handType, handLms in zip(self.results.multi_handedness, self.results.multi_hand_landmarks):\n",
    "                myHand = {}\n",
    "                ## lmList\n",
    "                mylmList = []\n",
    "                xList = []\n",
    "                yList = []\n",
    "                for id, lm in enumerate(handLms.landmark):\n",
    "                    px, py, pz = int(lm.x * w), int(lm.y * h), int(lm.z * w)\n",
    "                    mylmList.append([px, py, pz])\n",
    "                    xList.append(px)\n",
    "                    yList.append(py)\n",
    "\n",
    "                ## bbox\n",
    "                xmin, xmax = min(xList), max(xList)\n",
    "                ymin, ymax = min(yList), max(yList)\n",
    "                boxW, boxH = xmax - xmin, ymax - ymin\n",
    "                bbox = xmin, ymin, boxW, boxH\n",
    "                cx, cy = bbox[0] + (bbox[2] // 2), \\\n",
    "                         bbox[1] + (bbox[3] // 2)\n",
    "\n",
    "                myHand[\"lmList\"] = mylmList\n",
    "                myHand[\"bbox\"] = bbox\n",
    "                myHand[\"center\"] = (cx, cy)\n",
    "\n",
    "                if flipType:\n",
    "                    if handType.classification[0].label == \"Right\":\n",
    "                        myHand[\"type\"] = \"Left\"\n",
    "                    else:\n",
    "                        myHand[\"type\"] = \"Right\"\n",
    "                else:\n",
    "                    myHand[\"type\"] = handType.classification[0].label\n",
    "                allHands.append(myHand)\n",
    "\n",
    "                ## draw\n",
    "                if draw:\n",
    "                    self.mpDraw.draw_landmarks(img, handLms,\n",
    "                                               self.mpHands.HAND_CONNECTIONS)\n",
    "#                     cv2.rectangle(img, (bbox[0] - 20, bbox[1] - 20),\n",
    "#                                   (bbox[0] + bbox[2] + 20, bbox[1] + bbox[3] + 20),\n",
    "#                                   (255, 0, 255), 2)\n",
    "#                     cv2.putText(img, myHand[\"type\"], (bbox[0] - 30, bbox[1] - 30), cv2.FONT_HERSHEY_PLAIN,\n",
    "#                                 2, (255, 0, 255), 2)\n",
    "        if draw:\n",
    "            return allHands,img\n",
    "        else:\n",
    "            return allHands\n",
    "\n",
    "    def fingersUp(self, myHand):\n",
    "        \"\"\"\n",
    "        Finds how many fingers are open and returns in a list.\n",
    "        Considers left and right hands separately\n",
    "        :return: List of which fingers are up\n",
    "        \"\"\"\n",
    "        myHandType = myHand[\"type\"]\n",
    "        myLmList = myHand[\"lmList\"]\n",
    "        if self.results.multi_hand_landmarks:\n",
    "            fingers = []\n",
    "            # Thumb\n",
    "            if myHandType == \"Right\":\n",
    "                if myLmList[self.tipIds[0]][0] > myLmList[self.tipIds[0] - 1][0]:\n",
    "                    fingers.append(1)\n",
    "                else:\n",
    "                    fingers.append(0)\n",
    "            else:\n",
    "                if myLmList[self.tipIds[0]][0] < myLmList[self.tipIds[0] - 1][0]:\n",
    "                    fingers.append(1)\n",
    "                else:\n",
    "                    fingers.append(0)\n",
    "\n",
    "            # 4 Fingers\n",
    "            for id in range(1, 5):\n",
    "                if myLmList[self.tipIds[id]][1] < myLmList[self.tipIds[id] - 2][1]:\n",
    "                    fingers.append(1)\n",
    "                else:\n",
    "                    fingers.append(0)\n",
    "        return fingers\n",
    "\n",
    "    def findDistance(self, p1, p2, img=None):\n",
    "        \"\"\"\n",
    "        Find the distance between two landmarks based on their\n",
    "        index numbers.\n",
    "        :param p1: Point1\n",
    "        :param p2: Point2\n",
    "        :param img: Image to draw on.\n",
    "        :param draw: Flag to draw the output on the image.\n",
    "        :return: Distance between the points\n",
    "                 Image with output drawn\n",
    "                 Line information\n",
    "        \"\"\"\n",
    "\n",
    "        x1, y1 = p1\n",
    "        x2, y2 = p2\n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "        length = math.hypot(x2 - x1, y2 - y1)\n",
    "        info = (x1, y1, x2, y2, cx, cy)\n",
    "        if img is not None:\n",
    "            cv2.circle(img, (x1, y1), 15, (255, 0, 255), cv2.FILLED)\n",
    "            cv2.circle(img, (x2, y2), 15, (255, 0, 255), cv2.FILLED)\n",
    "            cv2.line(img, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
    "            cv2.circle(img, (cx, cy), 15, (255, 0, 255), cv2.FILLED)\n",
    "            return length, info, img\n",
    "        else:\n",
    "            return length, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8303dd2a",
   "metadata": {},
   "source": [
    "# Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be7eafc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 900 images belonging to 15 classes.\n",
      "Found 180 images belonging to 9 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['drink',\n",
       " 'food',\n",
       " 'full',\n",
       " 'have',\n",
       " 'hello',\n",
       " 'i',\n",
       " 'i love you',\n",
       " 'police',\n",
       " 'prefer',\n",
       " 'shirt',\n",
       " 'telephone',\n",
       " 'water',\n",
       " 'wrong',\n",
       " 'yes',\n",
       " 'you']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train = ImageDataGenerator(rescale = 1./255\n",
    "#)\n",
    "##Normalizing the inputs\n",
    "#validation = ImageDataGenerator(rescale = 1./255)\n",
    "#\n",
    "##Splitting the dataset into trian and validation\n",
    "#train_dataset = train.flow_from_directory('C:/graduationProject2023/Dataset/trainSet',\n",
    "#                                         target_size = (350,350),\n",
    "#                                         batch_size = 32,\n",
    "#                                         class_mode = \"categorical\")\n",
    "#validation_dataset = validation.flow_from_directory('C:/graduationProject2023/croppedDataset/validSet',\n",
    "#                                                   target_size = (350,350),\n",
    "#                                                   batch_size = 32,\n",
    "#                                                   class_mode = \"categorical\")\n",
    "#train_dataset.class_indices\n",
    "#dic = list(train_dataset.class_indices)\n",
    "#dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20e8926",
   "metadata": {},
   "source": [
    "# Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ba73364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the model\n",
    "#model = load_model(\"C:/graduationProject2023/model/VGG16_Augmented1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241d1f0",
   "metadata": {},
   "source": [
    "## Real time capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c8c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "\n",
    "#Receiving input from live camera feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=1)\n",
    "offset = 10\n",
    "imgSize = 350\n",
    "\n",
    "word = \"\"\n",
    "oldWord = \"\"\n",
    "\n",
    "#Setting the prediction to 5 frames per second\n",
    "frame_rate = 5\n",
    "prev = 0\n",
    "\n",
    "labels = []\n",
    "word = {}\n",
    "\n",
    "#Counter for saving dataset collection\n",
    "counter = 0\n",
    "result = 0\n",
    "\n",
    "while True:\n",
    "    #Calculating the framerate\n",
    "    time_elapsed = time.time() - prev\n",
    "    res, image = cap.read()\n",
    "    try:\n",
    "        success, img = cap.read()\n",
    "        imgOutput = img.copy()\n",
    "        hands, img = detector.findHands(img)\n",
    "        #If hands are detected using MediaPipe\n",
    "        if hands:\n",
    "            #Since we only have 1 hand, we'll be using hands[0] not hands[1]\n",
    "            hand = hands[0]\n",
    "            #Setting the x and y from the bounding box as well as the width and height\n",
    "            x, y, w, h = hand['bbox']\n",
    "            imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255\n",
    "            #Cropping to the hand\n",
    "            imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]\n",
    "            imgCropShape = imgCrop.shape\n",
    "            aspectRatio = h / w\n",
    "            #Checking if the hand is vertical or horizontal\n",
    "            #and filling the empty spaces with white\n",
    "            if aspectRatio > 1:\n",
    "                k = imgSize / h\n",
    "                wCal = math.ceil(k * w)\n",
    "                imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                wGap = math.ceil((imgSize - wCal) / 2)\n",
    "                imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "            else:\n",
    "                k = imgSize / w\n",
    "                hCal = math.ceil(k * h)\n",
    "                imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                hGap = math.ceil((imgSize - hCal) / 2)\n",
    "                imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "            #Labels are stored in labels list\n",
    "            imgWhiteCopy = imgWhite.copy()\n",
    "            #Necessary processing for the image\n",
    "            imgWhite = img_to_array(imgWhite)\n",
    "            imgWhite = imgWhite.reshape((1, imgWhite.shape[0], imgWhite.shape[1], imgWhite.shape[2]))\n",
    "            imgWhite = preprocess_input(imgWhite)\n",
    "            #Checking to see if the time between frames has passed\n",
    "            if time_elapsed > 1./frame_rate:\n",
    "                prev = time.time()\n",
    "                #result = model.predict(imgWhite)\n",
    "                #word = dic[result.argmax()]\n",
    "                oldWord = word\n",
    "                #result = result * 100000000\n",
    "                #maxVal = (result[0].max()/sum(result[0])) * 100\n",
    "                maxVal = 100\n",
    "                #Checking to see if the prediction confidence is greater than 99.9999999\n",
    "                if(maxVal > 99.9999999):\n",
    "                    #Putting the result on the box\n",
    "                    #cv2.putText(imgOutput, dic[result.argmax()],(x,y-20), cv2.FONT_HERSHEY_DUPLEX,2,(255,0,255),2)\n",
    "                    cv2.imshow(f\"ImageCrop\", imgCrop)\n",
    "                else:\n",
    "                    #Putting an empty prediction\n",
    "                    #cv2.putText(imgOutput, \"\",(x,y-20), cv2.FONT_HERSHEY_DUPLEX,2,(255,0,255),2)\n",
    "                    cv2.imshow(f\"ImageCrop\", imgCrop)\n",
    "            else:\n",
    "                cv2.putText(imgOutput, word,(x,y-20), cv2.FONT_HERSHEY_DUPLEX,2,(255,0,255),2)\n",
    "            #cv2.rectangle(imgOutput, (x-offset, y-offset),\n",
    "                         # (x + w+offset, y + h+offset), (255, 0, 255), 4)\n",
    "    #            cv2.putText(imgOutput, dic[result.argmax()],(x,y-20), cv2.FONT_HERSHEY_COMPLEX,2,(255,0,255),2)\n",
    "    #            cv2.imshow(f\"ImageCrop\", imgCrop)\n",
    "           # print(imgWhite.shape)\n",
    "            cv2.imshow(f\"imgWhite\", imgWhiteCopy)\n",
    "        cv2.imshow(\"Image\", imgOutput)\n",
    "        key = cv2.waitKey(1)\n",
    "        #Saving the image if \"S\" is pressed\n",
    "        if key == ord(\"s\"):            \n",
    "            counter+=1\n",
    "            cv2.imwrite(f\"C:/Users/User/Desktop/user_no_imagename{counter}.jpg\", imgOutput)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfc53f5",
   "metadata": {},
   "source": [
    "# Single Image Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da61a3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "detector = HandDetector(maxHands=1)\n",
    "offset = 20\n",
    "imgSize = 350\n",
    "path = \"C:/graduationProject2023/croppedDataset/trainSet/i love you\"\n",
    "savedFolder = \"C:/graduationProject2023/Dataset/Cropped Images Generated\"\n",
    "\n",
    "#print(f\"Count:{count} -  {folderPath}/{folder}/{image}\")\n",
    "for folder in os.listdir(path):\n",
    "    counter =0\n",
    "    for img in os.listdir(f\"{path}/{folder}\"):\n",
    "        img = cv2.imread(f\"{path}/{folder}/{img}\")\n",
    "        imgOutput = img.copy()\n",
    "        hands, img = detector.findHands(img)\n",
    "        #cv2.imshow(f\"Image{count+1}\",img)\n",
    "        if hands:\n",
    "            hand = hands[0]\n",
    "            x, y, w, h = hand['bbox']\n",
    "            imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255\n",
    "            imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]\n",
    "            imgCropShape = imgCrop.shape\n",
    "            aspectRatio = h / w\n",
    "            if aspectRatio > 1:\n",
    "                k = imgSize / h\n",
    "                wCal = math.ceil(k * w)\n",
    "                imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                wGap = math.ceil((imgSize - wCal) / 2)\n",
    "                imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "            else:\n",
    "                k = imgSize / w\n",
    "                hCal = math.ceil(k * h)\n",
    "                imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                hGap = math.ceil((imgSize - hCal) / 2)\n",
    "                imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "            cv2.rectangle(imgOutput, (x - offset, y - offset-50),\n",
    "                          (x - offset+90, y - offset-50+50), (255, 0, 255), cv2.FILLED)\n",
    "            #Labels are stored in labels lis\n",
    "            #cv2.putText(imgOutput, labels[index], (x, y -26), cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)\n",
    "            cv2.rectangle(imgOutput, (x-offset, y-offset),\n",
    "                          (x + w+offset, y + h+offset), (255, 0, 255), 4)\n",
    "            #       cv2.imshow(\"ImageCrop\", imgCrop)\n",
    "            print(imgWhite.shape)\n",
    "            imgWhiteCopy = imgWhite.copy()\n",
    "\n",
    "            imgWhite = img_to_array(imgWhite)\n",
    "            imgWhite = imgWhite.reshape((1, imgWhite.shape[0], imgWhite.shape[1], imgWhite.shape[2]))\n",
    "            imgWhite = preprocess_input(imgWhite)\n",
    "            result = model.predict(imgWhite).argmax()\n",
    "            cv2.imwrite(f\"C:/graduationProject2023/croppedDataset/trainSet/i love you/i love you_11.jpg\", imgWhiteCopy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a15957",
   "metadata": {},
   "source": [
    "# Vertical Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f17797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "path = \"C:/graduationProject2023/croppedDataset/validSet/\"\n",
    "for folder in os.listdir(path):\n",
    "    for image in os.listdir(f\"{path}/{folder}\"):\n",
    "        img = cv2.imread(f\"{path}/{folder}/{image}\")\n",
    "        flipped = cv2.flip(img,1)\n",
    "        cv2.imwrite(f\"{path}/{folder}/{image}_flipped.jpg\",flipped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6adcc4",
   "metadata": {},
   "source": [
    "# Individual Image Examiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2899574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cv2\n",
    "#from cvzone.HandTrackingModule import HandDetector\n",
    "\n",
    "detector = HandDetector(maxHands =1)\n",
    "img = cv2.imread(f\"C:/graduationProject2023/Dataset/New_Images/i love you/WhatsApp Image 2022-12-07 at 18.03.04.jpg\")\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "hands, img = detector.findHands(img)\n",
    "cv2.imshow(\"Image\",img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eb8eaf",
   "metadata": {},
   "source": [
    "# Automatic Hand Cropper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "\n",
    "detector = HandDetector(maxHands=1)\n",
    "offset = 20\n",
    "imgSize = 350\n",
    "#folder = \"C:/graduationProject2023/Dataset/TrainSet/bed/\"\n",
    "folderPath = \"C:/graduationProject2023/Dataset/ValidationSet\"\n",
    "folderList = [\"bed\",\"father\",\"full\",\"police\",\"shirt\",\"water\",\"wrong\"]\n",
    "counter = 0\n",
    "for folder in folderList:\n",
    "    for count, image in enumerate(os.listdir(f\"{folderPath}/{folder}\")):\n",
    "        img = cv2.imread(f\"{folderPath}/{folder}/{image}\")\n",
    "        #print(f\"Count:{count} -  {folderPath}/{folder}/{image}\")\n",
    "        imgOutput = img.copy()\n",
    "        hands, img = detector.findHands(img)\n",
    "        #cv2.imshow(f\"Image{count+1}\",img)\n",
    "        if hands:\n",
    "            counter+=1\n",
    "            hand = hands[0]\n",
    "            x, y, w, h = hand['bbox']\n",
    "            imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255\n",
    "            imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]\n",
    "            imgCropShape = imgCrop.shape\n",
    "            aspectRatio = h / w\n",
    "            if aspectRatio > 1:\n",
    "                k = imgSize / h\n",
    "                wCal = math.ceil(k * w)\n",
    "                imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                wGap = math.ceil((imgSize - wCal) / 2)\n",
    "                imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "            else:\n",
    "                k = imgSize / w\n",
    "                hCal = math.ceil(k * h)\n",
    "                imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                hGap = math.ceil((imgSize - hCal) / 2)\n",
    "                imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "            cv2.rectangle(imgOutput, (x - offset, y - offset-50),\n",
    "                          (x - offset+90, y - offset-50+50), (255, 0, 255), cv2.FILLED)\n",
    "            #Labels are stored in labels list\n",
    "\n",
    "            #cv2.putText(imgOutput, labels[index], (x, y -26), cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)\n",
    "            cv2.rectangle(imgOutput, (x-offset, y-offset),\n",
    "                          (x + w+offset, y + h+offset), (255, 0, 255), 4)\n",
    "    #         cv2.imshow(\"ImageCrop\", imgCrop)\n",
    "            #cv2.imshow(\"ImageWhite\", imgWhite)\n",
    "            cv2.imwrite(f\"C:/graduationProject2023/croppedDataset/validSet/{image}\",imgWhite)\n",
    "        else:\n",
    "            cv2.imwrite(f\"C:/graduationProject2023/croppedDataset/testSet/{image}\",img)\n",
    "print(counter)\n",
    "\n",
    "# cv2.imshow(\"Image\", imgOutput)\n",
    "# cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfd0814",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"python {}/research/object_detection/exporter_main_v2.py --input_type=image_tensor --pipeline_config_path={}/{}/pipeline.config --trained_checkpoint_dir={} --output_directory={}export\"\"\".format(APIMODEL_PATH, MODEL_PATH, CUSTOM_MODEL_NAME,CHECKPOINT_PATH, CHECKPOINT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0491e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
