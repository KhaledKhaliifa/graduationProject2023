{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 1,
>>>>>>> e0bbdd7f13f0f21db168c73ea8407dff10b43aca
   "id": "ef1e58c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator , img_to_array, load_img\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 2,
>>>>>>> e0bbdd7f13f0f21db168c73ea8407dff10b43aca
   "id": "e9c3555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "\n",
    "\n",
    "class HandDetector:\n",
    "    \"\"\"\n",
    "    Finds Hands using the mediapipe library. Exports the landmarks\n",
    "    in pixel format. Adds extra functionalities like finding how\n",
    "    many fingers are up or the distance between two fingers. Also\n",
    "    provides bounding box info of the hand found.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode=False, maxHands=2, detectionCon=0.5, minTrackCon=0.5):\n",
    "        \"\"\"\n",
    "        :param mode: In static mode, detection is done on each image: slower\n",
    "        :param maxHands: Maximum number of hands to detect\n",
    "        :param detectionCon: Minimum Detection Confidence Threshold\n",
    "        :param minTrackCon: Minimum Tracking Confidence Threshold\n",
    "        \"\"\"\n",
    "        self.mode = mode\n",
    "        self.maxHands = maxHands\n",
    "        self.detectionCon = detectionCon\n",
    "        self.minTrackCon = minTrackCon\n",
    "\n",
    "        self.mpHands = mp.solutions.hands\n",
    "        self.hands = self.mpHands.Hands(static_image_mode=self.mode, max_num_hands=self.maxHands,\n",
    "                                        min_detection_confidence=self.detectionCon,\n",
    "                                        min_tracking_confidence=self.minTrackCon)\n",
    "        self.mpDraw = mp.solutions.drawing_utils\n",
    "        self.tipIds = [4, 8, 12, 16, 20]\n",
    "        self.fingers = []\n",
    "        self.lmList = []\n",
    "\n",
    "    def findHands(self, img, draw=True, flipType=True):\n",
    "        \"\"\"\n",
    "        Finds hands in a BGR image.\n",
    "        :param img: Image to find the hands in.\n",
    "        :param draw: Flag to draw the output on the image.\n",
    "        :return: Image with or without drawings\n",
    "        \"\"\"\n",
    "        #blank_image = np.zeros((1080,1080,3), np.uint8)\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        self.results = self.hands.process(imgRGB)\n",
    "        allHands = []\n",
    "        h, w, c = img.shape\n",
    "        if self.results.multi_hand_landmarks:\n",
    "            for handType, handLms in zip(self.results.multi_handedness, self.results.multi_hand_landmarks):\n",
    "                myHand = {}\n",
    "                ## lmList\n",
    "                mylmList = []\n",
    "                xList = []\n",
    "                yList = []\n",
    "                for id, lm in enumerate(handLms.landmark):\n",
    "                    px, py, pz = int(lm.x * w), int(lm.y * h), int(lm.z * w)\n",
    "                    mylmList.append([px, py, pz])\n",
    "                    xList.append(px)\n",
    "                    yList.append(py)\n",
    "\n",
    "                ## bbox\n",
    "                xmin, xmax = min(xList), max(xList)\n",
    "                ymin, ymax = min(yList), max(yList)\n",
    "                boxW, boxH = xmax - xmin, ymax - ymin\n",
    "                bbox = xmin, ymin, boxW, boxH\n",
    "                cx, cy = bbox[0] + (bbox[2] // 2), \\\n",
    "                         bbox[1] + (bbox[3] // 2)\n",
    "\n",
    "                myHand[\"lmList\"] = mylmList\n",
    "                myHand[\"bbox\"] = bbox\n",
    "                myHand[\"center\"] = (cx, cy)\n",
    "\n",
    "                if flipType:\n",
    "                    if handType.classification[0].label == \"Right\":\n",
    "                        myHand[\"type\"] = \"Left\"\n",
    "                    else:\n",
    "                        myHand[\"type\"] = \"Right\"\n",
    "                else:\n",
    "                    myHand[\"type\"] = handType.classification[0].label\n",
    "                allHands.append(myHand)\n",
    "\n",
    "                ## draw\n",
    "                if draw:\n",
    "                    self.mpDraw.draw_landmarks(img, handLms,\n",
    "                                               self.mpHands.HAND_CONNECTIONS)\n",
    "#                     cv2.rectangle(img, (bbox[0] - 20, bbox[1] - 20),\n",
    "#                                   (bbox[0] + bbox[2] + 20, bbox[1] + bbox[3] + 20),\n",
    "#                                   (255, 0, 255), 2)\n",
    "#                     cv2.putText(img, myHand[\"type\"], (bbox[0] - 30, bbox[1] - 30), cv2.FONT_HERSHEY_PLAIN,\n",
    "#                                 2, (255, 0, 255), 2)\n",
    "        if draw:\n",
    "            return allHands,img\n",
    "        else:\n",
    "            return allHands\n",
    "\n",
    "    def fingersUp(self, myHand):\n",
    "        \"\"\"\n",
    "        Finds how many fingers are open and returns in a list.\n",
    "        Considers left and right hands separately\n",
    "        :return: List of which fingers are up\n",
    "        \"\"\"\n",
    "        myHandType = myHand[\"type\"]\n",
    "        myLmList = myHand[\"lmList\"]\n",
    "        if self.results.multi_hand_landmarks:\n",
    "            fingers = []\n",
    "            # Thumb\n",
    "            if myHandType == \"Right\":\n",
    "                if myLmList[self.tipIds[0]][0] > myLmList[self.tipIds[0] - 1][0]:\n",
    "                    fingers.append(1)\n",
    "                else:\n",
    "                    fingers.append(0)\n",
    "            else:\n",
    "                if myLmList[self.tipIds[0]][0] < myLmList[self.tipIds[0] - 1][0]:\n",
    "                    fingers.append(1)\n",
    "                else:\n",
    "                    fingers.append(0)\n",
    "\n",
    "            # 4 Fingers\n",
    "            for id in range(1, 5):\n",
    "                if myLmList[self.tipIds[id]][1] < myLmList[self.tipIds[id] - 2][1]:\n",
    "                    fingers.append(1)\n",
    "                else:\n",
    "                    fingers.append(0)\n",
    "        return fingers\n",
    "\n",
    "    def findDistance(self, p1, p2, img=None):\n",
    "        \"\"\"\n",
    "        Find the distance between two landmarks based on their\n",
    "        index numbers.\n",
    "        :param p1: Point1\n",
    "        :param p2: Point2\n",
    "        :param img: Image to draw on.\n",
    "        :param draw: Flag to draw the output on the image.\n",
    "        :return: Distance between the points\n",
    "                 Image with output drawn\n",
    "                 Line information\n",
    "        \"\"\"\n",
    "\n",
    "        x1, y1 = p1\n",
    "        x2, y2 = p2\n",
    "        cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "        length = math.hypot(x2 - x1, y2 - y1)\n",
    "        info = (x1, y1, x2, y2, cx, cy)\n",
    "        if img is not None:\n",
    "            cv2.circle(img, (x1, y1), 15, (255, 0, 255), cv2.FILLED)\n",
    "            cv2.circle(img, (x2, y2), 15, (255, 0, 255), cv2.FILLED)\n",
    "            cv2.line(img, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
    "            cv2.circle(img, (cx, cy), 15, (255, 0, 255), cv2.FILLED)\n",
    "            return length, info, img\n",
    "        else:\n",
    "            return length, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8303dd2a",
   "metadata": {},
   "source": [
    "# Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "be7eafc8",
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 3,
   "id": "be7eafc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 438 images belonging to 11 classes.\n",
      "Found 31 images belonging to 7 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bed',\n",
       " 'father',\n",
       " 'full',\n",
       " 'hello',\n",
       " 'i love you',\n",
       " 'police',\n",
       " 'shirt',\n",
       " 'telephone',\n",
       " 'water',\n",
       " 'wrong',\n",
       " 'yes']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> e0bbdd7f13f0f21db168c73ea8407dff10b43aca
   "source": [
    "train = ImageDataGenerator(rescale = 1./255\n",
    ")\n",
    "validation = ImageDataGenerator(rescale = 1./255)\n",
    "train_dataset = train.flow_from_directory('C:/graduationProject2023/croppedDataset/trainSet',\n",
    "                                         target_size = (350,350),\n",
    "                                         batch_size = 32,\n",
    "                                         class_mode = \"categorical\")\n",
    "validation_dataset = validation.flow_from_directory('C:/graduationProject2023/croppedDataset/validSet',\n",
    "                                                   target_size = (350,350),\n",
    "                                                   batch_size = 32,\n",
    "                                                   class_mode = \"categorical\")\n",
    "train_dataset.class_indices\n",
    "dic = list(train_dataset.class_indices)\n",
    "dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20e8926",
   "metadata": {},
   "source": [
    "# Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba73364",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "model = load_model(\"C:/graduationProject2023/model/VGG19_Aug.h5\")"
=======
    "model = load_model(\"C:/graduationProject2023/model/vgg16_1.h5\")"
>>>>>>> e0bbdd7f13f0f21db168c73ea8407dff10b43aca
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241d1f0",
   "metadata": {},
   "source": [
    "## Real time capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86c8c35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2308\\2668110490.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimgSize\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mwCal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0mimgResize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgCrop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mwCal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mimgResizeShape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimgResize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mwGap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgSize\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mwCal\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.6.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4052: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=1)\n",
    "offset = 10\n",
    "imgSize = 350\n",
<<<<<<< HEAD
    "folder = \"C:/graduationProject2023/croppedDataset/trainSet/i love you\"\n",
    "word = \"i love you\"\n",
    "\n",
    "counter = 0\n",
=======
    "folder = \"C:/graduationProject2023/croppedDataset/trainSet/water\"\n",
    "word = \"water\"\n",
    "counter = 18\n",
>>>>>>> e0bbdd7f13f0f21db168c73ea8407dff10b43aca
    "result = 0\n",
    "while True:\n",
    "    try:\n",
    "        success, img = cap.read()\n",
    "        imgOutput = img.copy()\n",
    "        hands, img = detector.findHands(img)\n",
    "        if hands:\n",
    "            hand = hands[0]\n",
    "            x, y, w, h = hand['bbox']\n",
    "\n",
    "            imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255\n",
    "            imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]\n",
    "\n",
    "            imgCropShape = imgCrop.shape\n",
    "\n",
    "            aspectRatio = h / w\n",
    "\n",
    "            if aspectRatio > 1:\n",
    "                k = imgSize / h\n",
    "                wCal = math.ceil(k * w)\n",
    "                imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                wGap = math.ceil((imgSize - wCal) / 2)\n",
    "                imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "            else:\n",
    "                k = imgSize / w\n",
    "                hCal = math.ceil(k * h)\n",
    "                imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                hGap = math.ceil((imgSize - hCal) / 2)\n",
    "                imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "\n",
<<<<<<< HEAD
    "    #         cv2.rectangle(imgOutput, (x - offset, y - offset-50),\n",
    "    #                       (x - offset+90, y - offset-50+50), (255, 0, 255), cv2.FILLED)\n",
    "            #Labels are stored in labels list\n",
    "            imgWhiteCopy = imgWhite.copy()\n",
    "            imgWhite = img_to_array(imgWhite)\n",
    "            imgWhite = imgWhite.reshape((1, imgWhite.shape[0], imgWhite.shape[1], imgWhite.shape[2]))\n",
    "            imgWhite = preprocess_input(imgWhite)\n",
    "\n",
    "            result = model.predict(imgWhite)\n",
=======
    "#         cv2.rectangle(imgOutput, (x - offset, y - offset-50),\n",
    "#                       (x - offset+90, y - offset-50+50), (255, 0, 255), cv2.FILLED)\n",
    "        #Labels are stored in labels list\n",
    "        imgWhiteCopy = imgWhite.copy()\n",
    "        imgWhite = img_to_array(imgWhite)\n",
    "        imgWhite = imgWhite.reshape((1, imgWhite.shape[0], imgWhite.shape[1], imgWhite.shape[2]))\n",
    "        imgWhite = preprocess_input(imgWhite)\n",
    "        \n",
    "        #result = model.predict(imgWhite)\n",
    "        \n",
    "    \n",
    "        cv2.rectangle(imgOutput, (x-offset, y-offset),\n",
    "                      (x + w+offset, y + h+offset), (255, 0, 255), 4)\n",
    "\n",
    "       # cv2.putText(imgOutput, dic[result.argmax()],(x,y-20), cv2.FONT_HERSHEY_COMPLEX,2,(255,0,255),2)\n",
    "        cv2.imshow(f\"ImageCrop\", imgCrop)\n",
    "    \n",
    "       # print(imgWhite.shape)\n",
    "        cv2.imshow(f\"imgWhite\", imgWhiteCopy)\n",
    "#         dicSum = sum(result[0])\n",
    "#         for count,elem in enumerate(result[0]):\n",
    "#             print(f\"{dic[count]} : {elem/dicSum}\")\n",
    "#         print(\"\\n\\n\")\n",
    "        #cv2.putText(dic[result], label, (x, y -26), cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)\n",
>>>>>>> e0bbdd7f13f0f21db168c73ea8407dff10b43aca
    "\n",
    "\n",
    "            cv2.rectangle(imgOutput, (x-offset, y-offset),\n",
    "                          (x + w+offset, y + h+offset), (255, 0, 255), 4)\n",
    "\n",
    "            cv2.putText(imgOutput, dic[result.argmax()],(x,y-20), cv2.FONT_HERSHEY_COMPLEX,2,(255,0,255),2)\n",
    "            cv2.imshow(f\"ImageCrop\", imgCrop)\n",
    "\n",
    "           # print(imgWhite.shape)\n",
    "            cv2.imshow(f\"imgWhite\", imgWhiteCopy)\n",
    "    #         dicSum = sum(result[0])\n",
    "    #         for count,elem in enumerate(result[0]):\n",
    "    #             print(f\"{dic[count]} : {elem/dicSum}\")\n",
    "    #         print(\"\\n\\n\")\n",
    "            #cv2.putText(dic[result], label, (x, y -26), cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)\n",
    "\n",
    "\n",
    "        cv2.imshow(\"Image\", imgOutput)\n",
    "        key = cv2.waitKey(1)\n",
    "    #     if key == ord(\"s\"):\n",
    "    #         counter+=1\n",
    "    #         cv2.imwrite(f'C:/graduationProject2023/croppedDataset/trainSet/i love you/i love you_11.jpg',imgWhiteCopy)\n",
    "    #         print(counter)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfc53f5",
   "metadata": {},
   "source": [
    "# Single Image Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da61a3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "detector = HandDetector(maxHands=1)\n",
    "offset = 20\n",
    "imgSize = 350\n",
    "path = \"C:/graduationProject2023/croppedDataset/trainSet/i love you\"\n",
    "savedFolder = \"C:/graduationProject2023/Dataset/Cropped Images Generated\"\n",
    "\n",
    "#print(f\"Count:{count} -  {folderPath}/{folder}/{image}\")\n",
    "for folder in os.listdir(path):\n",
    "    counter =0\n",
    "    for img in os.listdir(f\"{path}/{folder}\"):\n",
    "        img = cv2.imread(f\"{path}/{folder}/{img}\")\n",
    "        imgOutput = img.copy()\n",
    "        hands, img = detector.findHands(img)\n",
    "        #cv2.imshow(f\"Image{count+1}\",img)\n",
    "        if hands:\n",
    "            hand = hands[0]\n",
    "            x, y, w, h = hand['bbox']\n",
    "            imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255\n",
    "            imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]\n",
    "            imgCropShape = imgCrop.shape\n",
    "            aspectRatio = h / w\n",
    "            if aspectRatio > 1:\n",
    "                k = imgSize / h\n",
    "                wCal = math.ceil(k * w)\n",
    "                imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                wGap = math.ceil((imgSize - wCal) / 2)\n",
    "                imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "            else:\n",
    "                k = imgSize / w\n",
    "                hCal = math.ceil(k * h)\n",
    "                imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                hGap = math.ceil((imgSize - hCal) / 2)\n",
    "                imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "            cv2.rectangle(imgOutput, (x - offset, y - offset-50),\n",
    "                          (x - offset+90, y - offset-50+50), (255, 0, 255), cv2.FILLED)\n",
    "            #Labels are stored in labels lis\n",
    "            #cv2.putText(imgOutput, labels[index], (x, y -26), cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)\n",
    "            cv2.rectangle(imgOutput, (x-offset, y-offset),\n",
    "                          (x + w+offset, y + h+offset), (255, 0, 255), 4)\n",
    "            #       cv2.imshow(\"ImageCrop\", imgCrop)\n",
    "            print(imgWhite.shape)\n",
    "            imgWhiteCopy = imgWhite.copy()\n",
    "\n",
    "            imgWhite = img_to_array(imgWhite)\n",
    "            imgWhite = imgWhite.reshape((1, imgWhite.shape[0], imgWhite.shape[1], imgWhite.shape[2]))\n",
    "            imgWhite = preprocess_input(imgWhite)\n",
    "            result = model.predict(imgWhite).argmax()\n",
    "            cv2.imwrite(f\"C:/graduationProject2023/croppedDataset/trainSet/i love you/i love you_11.jpg\", imgWhiteCopy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a15957",
   "metadata": {},
   "source": [
    "# Vertical Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f17797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "path = \"C:/graduationProject2023/croppedDataset/validSet/\"\n",
    "for folder in os.listdir(path):\n",
    "    for image in os.listdir(f\"{path}/{folder}\"):\n",
    "        img = cv2.imread(f\"{path}/{folder}/{image}\")\n",
    "        flipped = cv2.flip(img,1)\n",
    "        cv2.imwrite(f\"{path}/{folder}/{image}_flipped.jpg\",flipped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6adcc4",
   "metadata": {},
   "source": [
    "# Individual Image Examiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2899574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cv2\n",
    "#from cvzone.HandTrackingModule import HandDetector\n",
    "\n",
    "detector = HandDetector(maxHands =1)\n",
    "img = cv2.imread(f\"C:/graduationProject2023/Dataset/New_Images/i love you/WhatsApp Image 2022-12-07 at 18.03.04.jpg\")\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "hands, img = detector.findHands(img)\n",
    "cv2.imshow(\"Image\",img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eb8eaf",
   "metadata": {},
   "source": [
    "# Automatic Hand Cropper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab389a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "\n",
    "detector = HandDetector(maxHands=1)\n",
    "offset = 20\n",
    "imgSize = 350\n",
    "#folder = \"C:/graduationProject2023/Dataset/TrainSet/bed/\"\n",
    "folderPath = \"C:/graduationProject2023/Dataset/ValidationSet\"\n",
    "folderList = [\"bed\",\"father\",\"full\",\"police\",\"shirt\",\"water\",\"wrong\"]\n",
    "counter = 0\n",
    "for folder in folderList:\n",
    "    for count, image in enumerate(os.listdir(f\"{folderPath}/{folder}\")):\n",
    "        img = cv2.imread(f\"{folderPath}/{folder}/{image}\")\n",
    "        #print(f\"Count:{count} -  {folderPath}/{folder}/{image}\")\n",
    "        imgOutput = img.copy()\n",
    "        hands, img = detector.findHands(img)\n",
    "        #cv2.imshow(f\"Image{count+1}\",img)\n",
    "        if hands:\n",
    "            counter+=1\n",
    "            hand = hands[0]\n",
    "            x, y, w, h = hand['bbox']\n",
    "            imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255\n",
    "            imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]\n",
    "            imgCropShape = imgCrop.shape\n",
    "            aspectRatio = h / w\n",
    "            if aspectRatio > 1:\n",
    "                k = imgSize / h\n",
    "                wCal = math.ceil(k * w)\n",
    "                imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                wGap = math.ceil((imgSize - wCal) / 2)\n",
    "                imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "            else:\n",
    "                k = imgSize / w\n",
    "                hCal = math.ceil(k * h)\n",
    "                imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "                imgResizeShape = imgResize.shape\n",
    "                hGap = math.ceil((imgSize - hCal) / 2)\n",
    "                imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "            cv2.rectangle(imgOutput, (x - offset, y - offset-50),\n",
    "                          (x - offset+90, y - offset-50+50), (255, 0, 255), cv2.FILLED)\n",
    "            #Labels are stored in labels list\n",
    "\n",
    "            #cv2.putText(imgOutput, labels[index], (x, y -26), cv2.FONT_HERSHEY_COMPLEX, 1.7, (255, 255, 255), 2)\n",
    "            cv2.rectangle(imgOutput, (x-offset, y-offset),\n",
    "                          (x + w+offset, y + h+offset), (255, 0, 255), 4)\n",
    "    #         cv2.imshow(\"ImageCrop\", imgCrop)\n",
    "            #cv2.imshow(\"ImageWhite\", imgWhite)\n",
    "            cv2.imwrite(f\"C:/graduationProject2023/croppedDataset/validSet/{image}\",imgWhite)\n",
    "        else:\n",
    "            cv2.imwrite(f\"C:/graduationProject2023/croppedDataset/testSet/{image}\",img)\n",
    "print(counter)\n",
    "\n",
    "# cv2.imshow(\"Image\", imgOutput)\n",
    "# cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
