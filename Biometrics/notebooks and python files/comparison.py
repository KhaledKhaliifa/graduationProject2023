# -*- coding: utf-8 -*-
"""comparison

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nz0O1nppU-2kjiCai5q8_eiAxijoHuHN
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
import warnings
import pickle

warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/Projects/GP Project/biometric/csv_files/Uniform_users.csv")
df_l = pd.read_csv("/content/drive/MyDrive/Projects/GP Project/biometric/csv_files/Uniform_users_labeled.csv")

df.drop(columns=df.columns[0], axis=1, inplace = True)
df_l.drop(columns=df_l.columns[0], axis=1, inplace = True)

df.head()

df_l.head()

x = df.loc[:, df.columns != 'label']
y = df.label

x_l = df_l.loc[:, df_l.columns != 'label']
y_l = df_l.label

x_nl = df_l.loc[:, df_l.columns != 'label']
y_nl = df_l.label

x_nl = x_nl.apply(lambda iterator: ((iterator.max() - iterator)/(iterator.max() - iterator.min())).round(2))

# original
X_train, X_both, y_train, y_both = train_test_split(x, y, test_size = 0.2, random_state=3)
X_test, X_valid, y_test, y_valid = train_test_split(X_both, y_both, test_size = 0.5, random_state=3)
#labeled
X_train_L, X_both_L, y_train_L, y_both_L = train_test_split(x_l, y_l, test_size = 0.2, random_state=3)
X_test_L, X_valid_L, y_test_L, y_valid_L = train_test_split(X_both_L, y_both_L, test_size = 0.5, random_state=3)
#normalized Labeled
X_train_NL, X_both_NL, y_train_NL, y_both_NL = train_test_split(x_nl, y_nl, test_size = 0.2, random_state=3)
X_test_NL, X_valid_NL, y_test_NL, y_valid_NL = train_test_split(X_both_NL, y_both_NL, test_size = 0.5, random_state=3)

#original models
rf = pickle.load(open('/content/drive/MyDrive/Projects/GP Project/biometric/Models/random_forest.pickle', "rb"))
knn = pickle.load(open('/content/drive/MyDrive/Projects/GP Project/biometric/Models/knn.pickle', "rb"))
dtree = pickle.load(open('/content/drive/MyDrive/Projects/GP Project/biometric/Models/dtree.pickle', "rb"))
svc = pickle.load(open('/content/drive/MyDrive/Projects/GP Project/biometric/Models/svm.pickle', "rb"))
lr = pickle.load(open('/content/drive/MyDrive/Projects/GP Project/biometric/Models/lr.pickle', "rb"))

#labeled images models
rf_l = pickle.load(open('/content/drive/MyDrive/Projects/GP Project/biometric/Models/random_forest_label.pickle', "rb"))
knn_l = pickle.load(open('/content/drive/MyDrive/Projects/GP Project/biometric/Models/knn_label.pickle', "rb"))
dtree_l = pickle.load(open('/content/drive/MyDrive/Projects/GP Project/biometric/Models/dtree_label.pickle', "rb"))
svc_l = pickle.load(open('/content/drive/MyDrive/Projects/GP Project/biometric/Models/svm_label.pickle', "rb"))
lr_l = pickle.load(open('/content/drive/MyDrive/Projects/GP Project/biometric/Models/lr_label.pickle', "rb"))

#labeled iamges and normalized models
rf_nl = pickle.load(open('/content/drive/MyDrive/Projects/GP Project/biometric/Models/random_forest_nl.pickle', "rb"))
knn_nl = pickle.load(open('/content/drive/MyDrive/Projects/GP Project/biometric/Models/knn_nl.pickle', "rb"))
dtree_nl = pickle.load(open('/content/drive/MyDrive/Projects/GP Project/biometric/Models/dtree_nl.pickle', "rb"))
svc_nl = pickle.load(open('/content/drive/MyDrive/Projects/GP Project/biometric/Models/svm_nl.pickle', "rb"))
lr_nl = pickle.load(open('/content/drive/MyDrive/Projects/GP Project/biometric/Models/lr_nl.pickle', "rb"))

"""# Plotting

"""

y_pred1 = knn.predict(X_test)
y_pred1_l = knn_l.predict(X_test_L)
y_pred1_nl= knn_nl.predict(X_test_NL)

y_pred2 = rf.predict(X_test)
y_pred2_l = rf_l.predict(X_test_L)
y_pred2_nl= rf_nl.predict(X_test_NL)

y_pred3 = lr.predict(X_test)
y_pred3_l = lr_l.predict(X_test_L)
y_pred3_nl= lr_nl.predict(X_test_NL)

y_pred4 = dtree.predict(X_test)
y_pred4_l = dtree_l.predict(X_test_L)
y_pred4_nl= dtree_nl.predict(X_test_NL)

y_pred5 = svc.predict(X_test)
y_pred5_l = svc_l.predict(X_test_L)
y_pred5_nl= svc_nl.predict(X_test_NL)

#making a dictionary with each word in our dataset as a key
my_dic = {'User1': 1, 'User2': 2, 'User3': 3, 'User4': 4, 'User5': 5}

#making a list of the dataset words
dic = list(my_dic)
my_titles = ["KNN","Random Forest","Logistic","Decision Tree","SVM"]

# function to add value labels
def addlabels(x,y):
    for i in range(len(x)):
        plt.text(i, y[i]/2, round(y[i],2), ha = 'center')

#plotting the f1-score comparison
x = []
l = []
nl = []

outputs = [y_pred1, y_pred2, y_pred4, y_pred5, y_pred3]
outputs_l = [y_pred1_l, y_pred2_l, y_pred4_l, y_pred5_l, y_pred3_l]
outputs_nl = [y_pred1_nl, y_pred2_nl, y_pred4_nl, y_pred5_nl, y_pred3_nl]

my_titles = ["KNN","Random Forest","Decision Tree","SVM","logistic"]
for i in range(len(outputs)):
  x.append(f1_score(y_test, outputs[i], average='weighted'))

for i in range(len(outputs_l)):
  l.append(f1_score(y_test, outputs_l[i], average='weighted'))

for i in range(len(outputs_nl)):
  nl.append(f1_score(y_test, outputs_nl[i], average='weighted'))


x.sort(reverse=True)
plt.figure(figsize=(9,7)) 
plt.title("F1 Score Comparison")

X_axis = np.arange(5)
plt.bar(X_axis - 0.15, x, 0.15, label = 'Unlabeled Images', color="orange")
plt.bar(X_axis, l, 0.15, label = 'Labeled Images', color="green")
plt.bar(X_axis + 0.15, nl, 0.15, label = 'Labeled & Normalized', color="grey")
plt.xticks(X_axis, my_titles)  
plt.yticks(np.arange(0, 1, 0.1)) 

plt.legend()  
plt.xlabel('Models')  
plt.ylabel("F1 Score")

# getting precision and recall for all models
p =[]
r = []
pr = []

a = []
a_l = []
a_n = []

for i in range(len(outputs)):
  p.append(precision_recall_fscore_support(y_test, outputs[i], average='weighted'))
  a.append(accuracy_score(y_test, outputs[i]))

  
for i in range(len(outputs_l)):
  r.append(precision_recall_fscore_support(y_test, outputs_l[i], average='weighted'))
  a_l.append(accuracy_score(y_test, outputs_l[i]))

for i in range(len(outputs_nl)):
  pr.append(precision_recall_fscore_support(y_test, outputs_nl[i], average='weighted'))
  a_n.append(accuracy_score(y_test, outputs_nl[i]))

print(p)
print(r)
print(pr)

# precision and recall comparison
pre = []
pre_l = []
pre_nl = []
re = []
re_l = []
re_nl =[]

for i in range(5):
  pre.append(p[i][0])
  pre_l.append(r[i][0])
  pre_nl.append(pr[i][0])
  
  re.append(p[i][1])
  re_l.append(r[i][1])
  re_nl.append(pr[i][1])
print(re)

def specificity(all_outs):
  last_item = []
  for out in all_outs:
    sp = 0
    # Calculate the confusion matrix
    cm = confusion_matrix(y_test, out)
    classes = len(cm)
    spec = []

    for i in range(classes):
      # True negatives for the current class
      tn = sum(cm[j, j] for j in range(classes) if j != i)
    
      # False positives for the current class
      fp = sum(cm[i, j] for j in range(classes) if j != i)
        
      # Calculate specificity for the current class
      sp += (tn / (tn + fp))
      if i == 4:
        last_item.append(sp)
  # calculating specificity of the whole model
  result = list(map(lambda x: x/5, last_item))

  # spec.append(last_item/5)
  print(result)
  return result

spe = specificity(outputs)
spe_l = specificity(outputs_l)
spe_nl = specificity(outputs_nl)
# print(spe)

# plotting Comparison
label = ['Precision', 'Recall', 'Accuracy', 'Specificity']
all_list = [pre, pre_l, pre_nl, re, re_l, re_nl, a, a_l, a_n, spe, spe_l, spe_nl]
iterate = 0

for i in range(4):
  plt.figure(figsize=(12,7)) 
  plt.title(f"{label[i]} Comparison")

  X_axis = np.arange(5)
  plt.bar(X_axis - 0.15, all_list[iterate], 0.15, label = 'Unlabeled Images', color="#6d4b4b")
  iterate+=1
  plt.bar(X_axis, all_list[iterate], 0.15, label = 'Labeled Images', color="#e4bcad")
  iterate+=1
  plt.bar(X_axis + 0.15, all_list[iterate], 0.15, label = 'Labeled & Normalized', color="#466964")
  iterate+=1

  plt.xticks(X_axis, my_titles)  
  plt.yticks(np.arange(0, 1.1, 0.1)) 

  plt.legend()  
  plt.xlabel('Models')  
  plt.ylabel(f"{label[i]}")

